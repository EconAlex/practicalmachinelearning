---
title: "Practical ML Course Project - Final"
author: "Alexander Cruz"
date: "3/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(caret)
library(rpart.plot)
library(randomForest)
library(mlbench)
library(e1071)
library(ggplot2)
library(egg)
```

### Problem definition: What do we want to predict?

Every problem in Data Science starts off with a very specific and well-designed question. So, what is it that we're trying to predict and what are we predicting it with? 

To that end, my research question is **Is it possible to accurately predict activity correctness (or incorrectness) for unilateral dumbell bicep curls using movement data from waist, arm, forearm, and dumbell sensors?**

### Exploratory Data Analysis

```{r load data, echo = FALSE}

trainDataMain <- read.csv("C:/Users/pr51813/Downloads/pml-training.csv")
testDataMain <- read.csv("C:/Users/pr51813/Downloads/pml-testing.csv")


```

First, I print a simple table to observe the distribution of cases among the prediction variable, in this case *class*. I can see that no classe is particularly predominant over any other, so data is fairly balanced for prediction. 
```{r exploratory 1, echo = FALSE}
table(trainDataMain$classe,useNA = "always")

```

```{r initial exploratory, echo=FALSE}
# Eliminate variables which hace NAs for purpose of plotting the pairs
incompleteColumns <- lapply(trainDataMain[,1:ncol(trainDataMain)],function(x){any(is.na(x))})
completeColumns <- names(incompleteColumns[incompleteColumns==FALSE])

# Eliminate dates and windows
testy <- trainDataMain[,c("raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","new_window","num_window","user_name","classe")]
# table(testy$new_window,testy$classe)
# table(testy$new_window,testy$user_name)
# table(testy$num_window,testy$classe)

featuresVector1 <- completeColumns[!completeColumns %in% c("X", "user_name",
                                                           "raw_timestamp_part_1",
                                                           "raw_timestamp_part_2", 
                                                           "cvtd_timestamp", 
                                                           "new_window", "num_window", 
                                                           "classe")]

```

Next, I incorporate visual aides to get a quick scan of possible correlations or discrimination between classes using pairs plot. For this I eliminate variables with NAs. Also, time stamp variables and "window" variables were also dropeed as these seemed to be meta-feaures related to the measurement and appear to have little correlation to the target variable. 

Here I've only included a few as demonstration, to save space. Below I also include some notes I made while looking at these.

```{r exploratory pairs, echo = FALSE}
# Full version
# for(i in seq(1,length(featuresVector1),5)){
#   if(i<81){
#     plot.new()
#     par(cex=0.8, mai=c(0.1,0.1,0.2,0.1))
#     print(
#     featurePlot(trainDataMain[,featuresVector1[i:(i+4)]],  
#                 as.factor(trainDataMain$classe), 
#                 plot = "pairs", 
#                 label = TRUE, 
#                 label.size = 2, 
#                 interactive = TRUE,
#                 auto.key = list(columns = 5)))
#     
#     text(0.9,c(1,0.75,0.5,0.25,0),labels = c(featuresVector1[i:(i+4)]), cex = 0.8)
#   } else {
#     
#     
#     plot.new()
#     par(cex=0.8, mai=c(0.1,0.1,0.2,0.1))
#     print(
#     featurePlot(trainDataMain[,featuresVector1[i:length(featuresVector1)]],  
#                 as.factor(trainDataMain$classe), 
#                 plot = "pairs", 
#                 label = TRUE, 
#                 label.size = 2, 
#                 interactive = TRUE, 
#                 labels = c(featuresVector1[i:(i+4)]),
#                 auto.key = list(columns = 5)))
#     
#     text(0.9,c(1,0.75,0.5,0.25,0),labels = c(featuresVector1[i:(i+4)]), cex = 0.8)
#     
#   }
#   
# }

# Small version
print(
  featurePlot(trainDataMain[,featuresVector1[c(1:4,15)]],  
              as.factor(trainDataMain$classe), 
              plot = "pairs", 
              label = TRUE, 
              label.size = 2, 
              interactive = TRUE,
              auto.key = list(columns = 5)))

# text(0.9,c(1,0.75,0.5,0.25,0),labels = c(featuresVector1[1:5]), cex = 0.8)


```

Some notes on observing the pairs plots:

- Arm measurements appear to be all over the place with no discernable patters.

- Kurtosis and skewness measures seem to have a lot of zeroes.

- _gyros_dumbell_y_ and _gyros_dumbell_z_ also seem to make neat clusters.

- There might be some potential in the dumbell measurements.

- Forearme measures make interesting patterns.


Similarly, I evaluated density plots looking for any discnernable classe distributions driven by any of the explanatory variables. 

```{r exploratory density, echo = FALSE}
# Full version
# for(i in seq(1,length(featuresVector1),5)){
#   if(i<81){
#     plot.new()
#     par(cex=0.8, mai=c(0.1,0.1,0.2,0.1))
#     print(
#       featurePlot(trainDataMain[,featuresVector1],  
#                   as.factor(trainDataMain$classe), 
#                   plot = "density", 
#                   label = TRUE, 
#                   label.size = 2, 
#                   scales = list(x = list(relation="free"), 
#                                 y = list(relation="free")), 
#                   adjust = 1.5, 
#                   pch = "|", 
#                   layout = c(4, 1), 
#                   auto.key = list(columns = 5)))
#     
#     text(0.9,c(1,0.75,0.5,0.25,0),labels = c(featuresVector1[i:(i+4)]), cex = 0.8)
#   } else {
#     
#     
#     plot.new()
#     par(cex=0.8, mai=c(0.1,0.1,0.2,0.1))
#     print(
#       featurePlot(trainDataMain[,featuresVector1[i:(i+4)]],  
#                   as.factor(trainDataMain$classe), 
#                   plot = "density", 
#                   label = TRUE, 
#                   label.size = 2, 
#                   scales = list(x = list(relation="free"), 
#                                 y = list(relation="free")), 
#                   adjust = 1.5, 
#                   pch = "|", 
#                   layout = c(4, 1), 
#                   auto.key = list(columns = 5)))
#     
#     text(0.9,c(1,0.75,0.5,0.25,0),labels = c(featuresVector1[i:(i+4)]), cex = 0.8)
#     
#   }
#   
# }


# Small version
print(
  featurePlot(trainDataMain[,featuresVector1[1:3]],  
              as.factor(trainDataMain$classe), 
              plot = "density", 
              label = TRUE, 
              label.size = 2, 
              scales = list(x = list(relation="free"), 
                            y = list(relation="free")), 
              adjust = 1.5, 
              pch = "|", 
              layout = c(4, 1), 
              auto.key = list(columns = 3)))

```

Some notes on observing the density plots:

- Group E seems to have its very distinct own distribution for *amplitude_pitch_belt*, *var_total_accel_belt*, *stddev_pitch_belt*, and *var_pitch_belt*.

- All *amplitude_yaw* variations show almost no differenciation between classe distributions.

- *gyros_arm_x* appears to have a distinctively higher mean than the rest.

- Slightly more differenciation coming from the dumbell and forearm sensors. 


### Pre-processing

Next, I move on to preparing the data for modelling. I'd already identified some features I wanted to drop in the exploratory stage, now for pre-processing I ran **nearZeroVar** function to identify near-zero features. I also used a comnbination of **cor** function and **findCorrelation** to eliminate highly correlated features (corr > 0.9).

These were the near-zero feautres identified and removed:
```{r preprocessing, echo = FALSE}

# Near Zero Variables
nzvMetrics <- nearZeroVar(trainDataMain[,featuresVector1], saveMetrics = TRUE)

nzvTRUE <- nzvMetrics[nzvMetrics$nzv==TRUE,]

nzvInCompleteCases <- row.names(nzvTRUE)[which(row.names(nzvTRUE) %in% featuresVector1)]
nzvInCompleteCases
#  [1] "kurtosis_roll_belt"      "kurtosis_picth_belt"     "kurtosis_yaw_belt"      
#  [4] "skewness_roll_belt"      "skewness_roll_belt.1"    "skewness_yaw_belt"      
#  [7] "max_yaw_belt"            "min_yaw_belt"            "amplitude_yaw_belt"     
# [10] "kurtosis_roll_arm"       "kurtosis_picth_arm"      "kurtosis_yaw_arm"       
# [13] "skewness_roll_arm"       "skewness_pitch_arm"      "skewness_yaw_arm"       
# [16] "kurtosis_roll_dumbbell"  "kurtosis_picth_dumbbell" "kurtosis_yaw_dumbbell"  
# [19] "skewness_roll_dumbbell"  "skewness_pitch_dumbbell" "skewness_yaw_dumbbell"  
# [22] "max_yaw_dumbbell"        "min_yaw_dumbbell"        "amplitude_yaw_dumbbell" 
# [25] "kurtosis_roll_forearm"   "kurtosis_picth_forearm"  "kurtosis_yaw_forearm"   
# [28] "skewness_roll_forearm"   "skewness_pitch_forearm"  "skewness_yaw_forearm"   
# [31] "max_yaw_forearm"         "min_yaw_forearm"         "amplitude_yaw_forearm"  

# apply(trainDataMain[,nzvInCompleteCases],2, FUN = summary)

featuresVector2 <- featuresVector1[!featuresVector1 %in% row.names(nzvTRUE)]
```

And these were the highly correlated features identified and removed:
```{r preprocessing2, echo = FALSE}
# Correlated predictors
descrCor <-  cor(trainDataMain[,featuresVector2])
highCorPairs <- findCorrelation(descrCor, cutoff = 0.90)

featuresVector3 <- featuresVector2[-highCorPairs]
featuresVector2[highCorPairs]

# Linear combinations
descrLinearCombs <- findLinearCombos(trainDataMain[,featuresVector3])
# None

# Set model data set
trainDataModel <- trainDataMain[,c("classe", featuresVector3)]

```

Notes and observations during pre-processing:

- Some variables (mostly kurtosis and skewness) are in character class and a lot of missing values. This is probably due to divisions resulting in error messages.

- All other nzv variables have a lot of NAs.

- Variables with NAs seem to have NAs for exacty the same records

- There seems to be no relationship between the missing values and either the user or the excercise type.

Lastly, once satisfied with the exploration, cleaning, and feature selection, I proceed to partitioning the provided training data into sub-samples. Although a testing data has been provided for prediction, further partitioning the training sample will allow me to perform cross-validation and have a good estimate of what the prediction error will be on the prediction data. 

```{r validation sample, echo=FALSE}
set.seed(7714)
inValidation <- createDataPartition(y=trainDataMain$classe, 
                                    p=0.7, list=F)
inValidationTrainData <- trainDataMain[inValidation,c("classe",featuresVector3)]
outValidationTrainData <- trainDataMain[-inValidation,c("classe",featuresVector3)]


```

### Model Selection

I tried first a LOESS regression and corresponding bagged approach. However, decision trees seemed not only to improve accuracy but also more intuitive with the classification problem at hand. Below an explanation of the three decision tree approaches I tried. 

##### Simple Decision Tree

```{r decision tree, echo=FALSE}

treeModel <- train(classe ~ .,
                   data = inValidationTrainData, method = "rpart")

# rpart.plot(treeModel$finalModel)
treeModel
# CART 
# 
# 13737 samples
#    45 predictor
#     5 classes: 'A', 'B', 'C', 'D', 'E' 
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 
# Resampling results across tuning parameters:
# 
#   cp          Accuracy   Kappa     
#   0.02858305  0.5272551  0.39170144
#   0.03326213  0.5052644  0.36329247
#   0.06535449  0.3278195  0.07222682
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was cp = 0.02858305.
```

The out-of-sample performance for the single decision tree:

```{r, echo = FALSE}
# Predicting
treePredict <- predict(treeModel, newdata = outValidationTrainData)
confusionMatrix(treePredict,as.factor(outValidationTrainData$classe))
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction    A    B    C    D    E
#          A 1527  506  457  368  240
#          B   29  372   30  178  194
#          C   90  227  422  100  235
#          D   28   33  117  243   42
#          E    0    1    0   75  371
# 
# Overall Statistics
#                                           
#                Accuracy : 0.4987          
#                  95% CI : (0.4859, 0.5116)
#     No Information Rate : 0.2845          
#     P-Value [Acc > NIR] : < 2.2e-16       
#                                           
#                   Kappa : 0.3449          
#                                           
#  Mcnemar's Test P-Value : < 2.2e-16       
# 
# Statistics by Class:
# 
#                      Class: A Class: B Class: C Class: D Class: E
# Sensitivity            0.9122  0.32660  0.41131  0.25207  0.34288
# Specificity            0.6269  0.90919  0.86582  0.95529  0.98418
# Pos Pred Value         0.4929  0.46326  0.39292  0.52484  0.82998
# Neg Pred Value         0.9473  0.84908  0.87445  0.86702  0.86925
# Prevalence             0.2845  0.19354  0.17434  0.16381  0.18386
# Detection Rate         0.2595  0.06321  0.07171  0.04129  0.06304
# Detection Prevalence   0.5264  0.13645  0.18250  0.07867  0.07596
# Balanced Accuracy      0.7696  0.61789  0.63856  0.60368  0.66353

```

##### Bagged Decision Tree

As you can see, the simple decision tree does not perform very well, so we try the bagging approach for decision trees, or **treebag** method in caret. 
```{r decision tree bagged, echo=FALSE}

treebagModel <- train(classe ~ .,
                      data = inValidationTrainData, method = "treebag")

treebagModel
# Bagged CART 
# 
# 13737 samples
#    45 predictor
#     5 classes: 'A', 'B', 'C', 'D', 'E' 
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 
# Resampling results:
# 
#   Accuracy   Kappa    
#   0.9764527  0.9702141
```

The out-of-sample performance for the bagged decision tree:

```{r, echo = FALSE}
# Predicting
treebagPredict <- predict(treebagModel, newdata = outValidationTrainData)
confusionMatrix(treebagPredict,as.factor(outValidationTrainData$classe))
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction    A    B    C    D    E
#          A 1665   10    0    0    0
#          B    7 1124   12    1    5
#          C    1    3 1010    8    6
#          D    1    2    4  951    2
#          E    0    0    0    4 1069
# 
# Overall Statistics
#                                           
#                Accuracy : 0.9888          
#                  95% CI : (0.9858, 0.9913)
#     No Information Rate : 0.2845          
#     P-Value [Acc > NIR] : < 2.2e-16       
#                                           
#                   Kappa : 0.9858          
#                                           
#  Mcnemar's Test P-Value : NA              
# 
# Statistics by Class:
# 
#                      Class: A Class: B Class: C Class: D Class: E
# Sensitivity            0.9946   0.9868   0.9844   0.9865   0.9880
# Specificity            0.9976   0.9947   0.9963   0.9982   0.9992
# Pos Pred Value         0.9940   0.9782   0.9825   0.9906   0.9963
# Neg Pred Value         0.9979   0.9968   0.9967   0.9974   0.9973
# Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
# Detection Rate         0.2829   0.1910   0.1716   0.1616   0.1816
# Detection Prevalence   0.2846   0.1952   0.1747   0.1631   0.1823
# Balanced Accuracy      0.9961   0.9908   0.9904   0.9923   0.9936

```

##### Random Forest

The bagged CART algorithm provides a significant improvement over the simple CART. However, I tried to push it a little further with a Random Forest to see if I could improve, at all, in accuracy. 

```{r random forest, echo=FALSE}

randomforestModel <- train(classe ~ .,
                           data = inValidationTrainData, method = "rf")
# 
randomforestModel
# Random Forest 
# 
# 13737 samples
#    45 predictor
#     5 classes: 'A', 'B', 'C', 'D', 'E' 
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 
# Resampling results across tuning parameters:
# 
#   mtry  Accuracy   Kappa    
#    2    0.9866063  0.9830532
#   23    0.9883702  0.9852860
#   45    0.9782213  0.9724427
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was mtry = 23.
```

The out-of-sample performance for the random forest with default parameter selection:

```{r, echo = FALSE}
# Prediction on hold-out sample
randomforestPredict <- predict(randomforestModel, newdata = outValidationTrainData)
confusionMatrix(randomforestPredict,as.factor(outValidationTrainData$classe))
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction    A    B    C    D    E
#          A 1674    5    0    0    0
#          B    0 1133    7    0    0
#          C    0    1 1017    7    7
#          D    0    0    2  956    4
#          E    0    0    0    1 1071
# 
# Overall Statistics
#                                          
#                Accuracy : 0.9942         
#                  95% CI : (0.9919, 0.996)
#     No Information Rate : 0.2845         
#     P-Value [Acc > NIR] : < 2.2e-16      
#                                          
#                   Kappa : 0.9927         
#                                          
#  Mcnemar's Test P-Value : NA             
# 
# Statistics by Class:
# 
#                      Class: A Class: B Class: C Class: D Class: E
# Sensitivity            1.0000   0.9947   0.9912   0.9917   0.9898
# Specificity            0.9988   0.9985   0.9969   0.9988   0.9998
# Pos Pred Value         0.9970   0.9939   0.9855   0.9938   0.9991
# Neg Pred Value         1.0000   0.9987   0.9981   0.9984   0.9977
# Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
# Detection Rate         0.2845   0.1925   0.1728   0.1624   0.1820
# Detection Prevalence   0.2853   0.1937   0.1754   0.1635   0.1822
# Balanced Accuracy      0.9994   0.9966   0.9941   0.9952   0.9948

```

The Random Forest algorithm provides better accuracy, and does not appear to be overfitting as the out-of-sample accuracy tracks closely to the in-sample metrics.

### Parameter Tuning

Now that we have decided on using Random Forest, we can try tuning some hyperparameters to try to make it even better. For this particular algorithm we're using, we can tune the _mtry_ parameter which will set the maximum number of (random) variables to try at each new split. For this, we set the **search** argument in the **trainControl** function to 'random' and the **tuneLength** in the **train** function to 8. 

The idea behind this is that we want the algorithm to try a few more _mtry_ values than it did in the previous run where we had left the defaults. Since I have no particular insight into what value of _mtry_ could yield better accuracy, I let the algorithm try a few random ones and select among these the best one. 

Furthermore, for this tuned run I also decided to try my luck with k-fold cross-validation sampling, instead of the default bootstrap. The k-fold cross validation has been known to help reduce variance and the possibility of overfitting since it ensures that every observation plays a role in both training and testing. 

```{r random forest tuned, echo=FALSE}

# Set k-folds cross validation options
control <- trainControl(method="repeatedcv", 
                        number=10, 
                        repeats=3, 
                        search = "random")
mtry <- sqrt(ncol(inValidationTrainData))

set.seed(7714)
randomforestModel2 <- train(classe ~ .,
                            data = inValidationTrainData,
                            method = "rf",
                            metric = "Accuracy",
                            tuneLength  = 8,
                            trControl = control)
randomforestModel2
# Random Forest 
# 
# 13737 samples
#    45 predictor
#     5 classes: 'A', 'B', 'C', 'D', 'E' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold, repeated 3 times) 
# Summary of sample sizes: 12364, 12363, 12363, 12363, 12364, 12363, ... 
# Resampling results across tuning parameters:
# 
#   mtry  Accuracy   Kappa    
#    4    0.9924043  0.9903909
#    6    0.9933992  0.9916497
#    7    0.9935689  0.9918644
#   18    0.9932776  0.9914957
#   27    0.9919917  0.9898688
#   31    0.9916033  0.9893772
#   38    0.9899779  0.9873207
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was mtry = 7.
```

The out-of-sample performance for the random forest with tuned hyperparameters:

```{r, echo = FALSE}
# Prediction on hold-out sample
randomforestPredict2 <- predict(randomforestModel2,newdata = outValidationTrainData)

confusionMatrix(randomforestPredict2,as.factor(outValidationTrainData$classe))
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction    A    B    C    D    E
#          A 1674    0    0    0    0
#          B    0 1137    5    0    0
#          C    0    2 1021    6    5
#          D    0    0    0  957    2
#          E    0    0    0    1 1075
# 
# Overall Statistics
#                                           
#                Accuracy : 0.9964          
#                  95% CI : (0.9946, 0.9978)
#     No Information Rate : 0.2845          
#     P-Value [Acc > NIR] : < 2.2e-16       
#                                           
#                   Kappa : 0.9955          
#                                           
#  Mcnemar's Test P-Value : NA              
# 
# Statistics by Class:
# 
#                      Class: A Class: B Class: C Class: D Class: E
# Sensitivity            1.0000   0.9982   0.9951   0.9927   0.9935
# Specificity            1.0000   0.9989   0.9973   0.9996   0.9998
# Pos Pred Value         1.0000   0.9956   0.9874   0.9979   0.9991
# Neg Pred Value         1.0000   0.9996   0.9990   0.9986   0.9985
# Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
# Detection Rate         0.2845   0.1932   0.1735   0.1626   0.1827
# Detection Prevalence   0.2845   0.1941   0.1757   0.1630   0.1828
# Balanced Accuracy      1.0000   0.9986   0.9962   0.9962   0.9967

```

We can see that hypertuning in this case has yielded a very small increase in accuracy. This will be the model we'll use for predicting the classe for the 20 test cases. 

### Prediction 

As mentioned above, the tuned Random Forest model will be used to predict over the 20-case test sample provided. Below is the outcome of predicted classes for these.  

```{r prediction, echo=FALSE}

predictOut <- predict(randomforestModel2,newdata = testDataMain)

outputLong <- data.frame(testDataMain, predictOut)

outputShort <- data.frame(testDataMain$problem_id, predictOut)


# write.csv(outputLong,"C:/Users/pr51813/OneDrive - POPULAR INC/Practical ML Course/prediction.csv", 
#           row.names = FALSE)

outputShort

```










